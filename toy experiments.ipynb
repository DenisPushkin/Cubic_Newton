{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cubic_subproblem_solver import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t: np.array):\n",
    "    # if t.dtype is integer, computation will fail\n",
    "    assert t.dtype == np.float64 or t.dtype == np.float32\n",
    "    mask = t >= 0\n",
    "    t[mask] = 1 / (1 + np.exp(-t[mask]))\n",
    "    t[~mask] = 1 - 1 / (1 + np.exp(t[~mask]))\n",
    "    return t\n",
    "\n",
    "\n",
    "def train_Cubic_Newton(X, y, w0, n_iters, M_0):\n",
    "    M = M_0\n",
    "    w = w0\n",
    "    for i in range(n_iters):\n",
    "        loss_ = loss(X,y,w)\n",
    "        grad = gradient(X,y,w)\n",
    "        hess = hessian(X, y, w)\n",
    "        h = cubic_subproblem_solver(grad, hess, M)\n",
    "        w_next = w + h\n",
    "        while (loss(X,y,w_next) > quadratic_form(loss_,grad,hess,M,w,w_next)):\n",
    "            M *= 2\n",
    "            h = cubic_subproblem_solver(grad, hess, M)\n",
    "            w_next = w + h\n",
    "        w = w_next\n",
    "        M /= 2\n",
    "        print(f'iter {i+1: 3,d}: loss = {loss(X,y,w): .5f}, M = {M}')\n",
    "    return w\n",
    "\n",
    "\n",
    "def train_GD(X, y, w0, n_iters, lr):\n",
    "    w = w0\n",
    "    for i in range(n_iters):\n",
    "        grad = gradient(X,y,w)\n",
    "        w = w - lr * grad\n",
    "        print(f'iter {i+1: 4,d}: loss = {loss(X,y,w): .5f}')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with cubic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(w) = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - x_i^T w|^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "\n",
    "X = np.random.uniform(-10, 10, (1000, 5))\n",
    "X = np.insert(X,-1,values=1,axis=1) #add bias\n",
    "w_true = np.array([-2.73, 3.14, 2.71, -1.12, -4.56, 0.51])\n",
    "score = X @ w_true\n",
    "noise = np.random.randn(*score.shape) * 5\n",
    "y = score + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w):\n",
    "    return np.mean(np.abs(y - X@w)**3)\n",
    "\n",
    "def gradient(X, y, w):\n",
    "    N = X.shape[0]\n",
    "    err = y - X@w\n",
    "    return (-3/N) * X.T @ (err**2 * np.sign(err))\n",
    "\n",
    "def hessian(X, y, w):\n",
    "    N = X.shape[0]\n",
    "    err = y - X@w\n",
    "    return (6/N) * X.T @ np.diag(err*np.sign(err)) @ X\n",
    "\n",
    "def quadratic_form(loss_w, grad_w, hess_w, M, w, w_new):\n",
    "    diff = w_new - w\n",
    "    return loss_w + np.dot(grad_w,diff) + 0.5 * np.dot(hess_w @ diff, diff) + M/6 * np.linalg.norm(diff,2)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   1: loss =  5564.58477, M = 0.005\n",
      "iter   2: loss =  919.95181, M = 0.0025\n",
      "iter   3: loss =  268.41898, M = 0.00125\n",
      "iter   4: loss =  180.54423, M = 0.000625\n",
      "iter   5: loss =  176.92169, M = 0.0003125\n",
      "iter   6: loss =  176.92102, M = 0.00015625\n",
      "iter   7: loss =  176.92102, M = 7.8125e-05\n",
      "iter   8: loss =  176.92102, M = 3.90625e-05\n",
      "iter   9: loss =  176.92102, M = 87960930222.08\n",
      "iter  10: loss =  176.92102, M = 87960930222.08\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros(X.shape[1])\n",
    "M_0 = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "w = train_Cubic_Newton(X, y, w0, n_iters, M_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    1: loss =  8208.11353\n",
      "iter    2: loss =  3904.21992\n",
      "iter    3: loss =  2363.10757\n",
      "iter    4: loss =  1633.48935\n",
      "iter    5: loss =  1231.67801\n",
      "iter    6: loss =  987.67753\n",
      "iter    7: loss =  828.91429\n",
      "iter    8: loss =  720.27715\n",
      "iter    9: loss =  643.05895\n",
      "iter   10: loss =  586.52905\n",
      "iter   11: loss =  544.16046\n",
      "iter   12: loss =  511.78763\n",
      "iter   13: loss =  486.64608\n",
      "iter   14: loss =  466.84208\n",
      "iter   15: loss =  451.04535\n",
      "iter   16: loss =  438.30219\n",
      "iter   17: loss =  427.91671\n",
      "iter   18: loss =  419.37209\n",
      "iter   19: loss =  412.27667\n",
      "iter   20: loss =  406.32990\n",
      "iter   21: loss =  401.29925\n",
      "iter   22: loss =  397.00327\n",
      "iter   23: loss =  393.29953\n",
      "iter   24: loss =  390.07550\n",
      "iter   25: loss =  387.24150\n",
      "iter   26: loss =  384.72570\n",
      "iter   27: loss =  382.47036\n",
      "iter   28: loss =  380.42891\n",
      "iter   29: loss =  378.56362\n",
      "iter   30: loss =  376.84382\n",
      "iter   31: loss =  375.24456\n",
      "iter   32: loss =  373.74549\n",
      "iter   33: loss =  372.32996\n",
      "iter   34: loss =  370.98435\n",
      "iter   35: loss =  369.69750\n",
      "iter   36: loss =  368.46021\n",
      "iter   37: loss =  367.26495\n",
      "iter   38: loss =  366.10548\n",
      "iter   39: loss =  364.97667\n",
      "iter   40: loss =  363.87426\n",
      "iter   41: loss =  362.79475\n",
      "iter   42: loss =  361.73523\n",
      "iter   43: loss =  360.69329\n",
      "iter   44: loss =  359.66691\n",
      "iter   45: loss =  358.65443\n",
      "iter   46: loss =  357.65446\n",
      "iter   47: loss =  356.66584\n",
      "iter   48: loss =  355.68760\n",
      "iter   49: loss =  354.71893\n",
      "iter   50: loss =  353.75915\n",
      "iter   51: loss =  352.80767\n",
      "iter   52: loss =  351.86402\n",
      "iter   53: loss =  350.92779\n",
      "iter   54: loss =  349.99864\n",
      "iter   55: loss =  349.07625\n",
      "iter   56: loss =  348.16039\n",
      "iter   57: loss =  347.25084\n",
      "iter   58: loss =  346.34741\n",
      "iter   59: loss =  345.44993\n",
      "iter   60: loss =  344.55827\n",
      "iter   61: loss =  343.67230\n",
      "iter   62: loss =  342.79191\n",
      "iter   63: loss =  341.91701\n",
      "iter   64: loss =  341.04751\n",
      "iter   65: loss =  340.18334\n",
      "iter   66: loss =  339.32441\n",
      "iter   67: loss =  338.47067\n",
      "iter   68: loss =  337.62206\n",
      "iter   69: loss =  336.77852\n",
      "iter   70: loss =  335.93999\n",
      "iter   71: loss =  335.10644\n",
      "iter   72: loss =  334.27782\n",
      "iter   73: loss =  333.45407\n",
      "iter   74: loss =  332.63517\n",
      "iter   75: loss =  331.82106\n",
      "iter   76: loss =  331.01172\n",
      "iter   77: loss =  330.20710\n",
      "iter   78: loss =  329.40717\n",
      "iter   79: loss =  328.61189\n",
      "iter   80: loss =  327.82123\n",
      "iter   81: loss =  327.03515\n",
      "iter   82: loss =  326.25363\n",
      "iter   83: loss =  325.47663\n",
      "iter   84: loss =  324.70411\n",
      "iter   85: loss =  323.93605\n",
      "iter   86: loss =  323.17241\n",
      "iter   87: loss =  322.41317\n",
      "iter   88: loss =  321.65829\n",
      "iter   89: loss =  320.90775\n",
      "iter   90: loss =  320.16151\n",
      "iter   91: loss =  319.41955\n",
      "iter   92: loss =  318.68184\n",
      "iter   93: loss =  317.94834\n",
      "iter   94: loss =  317.21904\n",
      "iter   95: loss =  316.49390\n",
      "iter   96: loss =  315.77289\n",
      "iter   97: loss =  315.05599\n",
      "iter   98: loss =  314.34317\n",
      "iter   99: loss =  313.63441\n",
      "iter  100: loss =  312.92967\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros(X.shape[1])\n",
    "n_iters = 100\n",
    "lr = 1e-4 # 1e-3 already diverges\n",
    "w = train_GD(X, y, w0, n_iters, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathcal{L}(w) = \\frac{1}{N} \\sum_{i=1}^N \\log \\left( 1 + \\exp(-y_i x_i^T w) \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 samples of positive class and 510 samples of negative class\n"
     ]
    }
   ],
   "source": [
    "# Creating dataset\n",
    "\n",
    "X = np.random.uniform(-10, 10, (1000, 5))\n",
    "w_true = np.array([3.14, 2.71, -1.12, -4.56, 0.51])\n",
    "score = X @ w_true\n",
    "noise = np.random.randn(*score.shape) * 25\n",
    "y = np.sign(score + noise)\n",
    "\n",
    "c1 = y[y == 1].size\n",
    "c2 = y[y == -1].size\n",
    "print(f'{c1} samples of positive class and {c2} samples of negative class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w):\n",
    "    return np.mean(- np.log(sigmoid(y * (X@w))))\n",
    "\n",
    "def gradient(X, y, w):\n",
    "    N = X.shape[0]\n",
    "    return (1/N) * X.T @ (-y * sigmoid(-y * (X@w)))\n",
    "\n",
    "def hessian(X, y, w):\n",
    "    N = X.shape[0]\n",
    "    probs = sigmoid(-y*(X@w))\n",
    "    return (1/N) * X.T @ np.diag(probs*(1-probs)) @ X\n",
    "\n",
    "def quadratic_form(loss_w, grad_w, hess_w, M, w, w_new):\n",
    "    diff = w_new - w\n",
    "    return loss_w + np.dot(grad_w,diff) + 0.5 * np.dot(hess_w @ diff, diff) + M/6 * np.linalg.norm(diff,2)**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   1: loss =  0.43857, M = 0.005\n",
      "iter   2: loss =  0.40460, M = 0.0025\n",
      "iter   3: loss =  0.40068, M = 0.00125\n",
      "iter   4: loss =  0.40060, M = 0.000625\n",
      "iter   5: loss =  0.40060, M = 0.0003125\n",
      "iter   6: loss =  0.40060, M = 0.005\n",
      "iter   7: loss =  0.40060, M = 0.0025\n",
      "iter   8: loss =  0.40060, M = 0.00125\n",
      "iter   9: loss =  0.40060, M = 0.000625\n",
      "iter  10: loss =  0.40060, M = 0.0003125\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros(X.shape[1])\n",
    "M_0 = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "w = train_Cubic_Newton(X, y, w0, n_iters, M_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    1: loss =  0.45788\n",
      "iter    2: loss =  0.42787\n",
      "iter    3: loss =  0.41595\n",
      "iter    4: loss =  0.40996\n",
      "iter    5: loss =  0.40658\n",
      "iter    6: loss =  0.40454\n",
      "iter    7: loss =  0.40326\n",
      "iter    8: loss =  0.40242\n",
      "iter    9: loss =  0.40186\n",
      "iter   10: loss =  0.40148\n",
      "iter   11: loss =  0.40122\n",
      "iter   12: loss =  0.40104\n",
      "iter   13: loss =  0.40091\n",
      "iter   14: loss =  0.40082\n",
      "iter   15: loss =  0.40076\n",
      "iter   16: loss =  0.40071\n",
      "iter   17: loss =  0.40068\n",
      "iter   18: loss =  0.40066\n",
      "iter   19: loss =  0.40064\n",
      "iter   20: loss =  0.40063\n",
      "iter   21: loss =  0.40062\n",
      "iter   22: loss =  0.40061\n",
      "iter   23: loss =  0.40061\n",
      "iter   24: loss =  0.40061\n",
      "iter   25: loss =  0.40060\n",
      "iter   26: loss =  0.40060\n",
      "iter   27: loss =  0.40060\n",
      "iter   28: loss =  0.40060\n",
      "iter   29: loss =  0.40060\n",
      "iter   30: loss =  0.40060\n",
      "iter   31: loss =  0.40060\n",
      "iter   32: loss =  0.40060\n",
      "iter   33: loss =  0.40060\n",
      "iter   34: loss =  0.40060\n",
      "iter   35: loss =  0.40060\n",
      "iter   36: loss =  0.40060\n",
      "iter   37: loss =  0.40060\n",
      "iter   38: loss =  0.40060\n",
      "iter   39: loss =  0.40060\n",
      "iter   40: loss =  0.40060\n",
      "iter   41: loss =  0.40060\n",
      "iter   42: loss =  0.40060\n",
      "iter   43: loss =  0.40060\n",
      "iter   44: loss =  0.40060\n",
      "iter   45: loss =  0.40060\n",
      "iter   46: loss =  0.40060\n",
      "iter   47: loss =  0.40060\n",
      "iter   48: loss =  0.40060\n",
      "iter   49: loss =  0.40060\n",
      "iter   50: loss =  0.40060\n"
     ]
    }
   ],
   "source": [
    "w0 = np.zeros(X.shape[1])\n",
    "n_iters = 50\n",
    "lr = 0.1\n",
    "w = train_GD(X, y, w0, n_iters, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
